# Fully_Convolutional_Network_Time_Series
使用全卷积神经网络处理时序数据
<<<<<<< HEAD

# 《Time Series Classification from Scratch with Deep Netural Networks: A Strong Baseline》论文总结

## 综述：
在论文中，主要论述了使用新的三种方法来对时序数据进行分类，这三种方法均是基于深度学习，与原来的distance-based基于距离和feature-based基于特征的方法不同，深度学习思想的方法在实验结果、模型性能等方面较前二者相比都有着较大的提升。

这三种方法分别是Multilayer Perceptrons多层感知机、Fully Ｃonvolutional Networks 全卷积神经网络、Residual Network 残差网络。

---
### 多层感知机

论文中，此模型包括三个全连接层，每层上包含500个神经元，同时在多层感知机结构的基础上，还设置了Dropout和Relu激活函数。Dropout设置的作用是能够防止深度较大的模型在非常小的数据集上面产生过拟合现象，而Relu激活函数的设置，能够使模型在很深的情况下防止出现梯度饱和的问题。

### 全卷积神经网络
全卷积神经网络在图片分割方面展现出非常好的性能。在本论文中，全卷积神经网络被用作特征提取器，最终的输出仍然需要经过Softmax层得出结果。全卷积神经网络基本的模块构成为——卷积层、Batch_Normalization层、Relu激活函数层，其中卷积层使用一维卷积实现。那么对于这样的三者合一的基本模块，全卷积神经网络总共包含三个，其中卷积层的卷积核个数分别为：128、256、128，除此之外，全卷积网络模型不包含池化层，仅仅在最后输出时将数据再放入全局池化层。这样的做法也在后一种模型（残差网络）中使用。

BN层的设置，能够加快模型拟合数据的收敛速度，同时改善模型的泛化能力，在三层全卷积快之后，紧接着是全局池化层，而不是传统意义的全连接层，这样的设置能够极大的减少参数的个数。模型的最后一层仍然是softmax层。

### 残差网络
参差网络通过在每一个残差块之间添加快捷连接（shortcut），将神经网络扩展成更加深的结构，这样能够使求解的梯度流直接通过网络底层。其在目标检测和其他视觉相关的任务中均表现出非常好的效果。残差网络的基本模块是在卷积网络的基础上构造的，比如说一个Res_Block便是由卷积层+BN+Relu这样组合构成，该论文中每三个这样的组合便组成了一个残差块，共有三个残差块，其中卷积核的个数分别为64、128、128，最后两层仍然是全局池化和SoftMax层。
=======
>>>>>>> 90200e08ccc8078e6456de3c9d879e4da2e082cf
